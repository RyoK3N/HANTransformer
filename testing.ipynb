{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing data directory for storing preprocessed data\n",
    "import shutil \n",
    "shutil.rmtree('/home/ubuntu/Research/Topic_Modelling/SLM2_v2/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 Newsgroups dataset...\n",
      "Tokenizing texts into sentences and words...\n",
      "Tokenizing: 100%|█████████████████████████| 18846/18846 [05:44<00:00, 54.64it/s]\n",
      "Building vocabularies...\n",
      "Word Vocabulary Size: 28017\n",
      "POS Vocabulary Size: 19\n",
      "Rule Vocabulary Size: 4\n",
      "Encoding texts...\n",
      "Splitting data into train and test sets...\n",
      "Saving processed data...\n",
      "Preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([2, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "HANTransformer                                     [2, 20]                   --\n",
       "├─Embedding: 1-1                                   [64, 128, 100]            3,052,200\n",
       "├─Embedding: 1-2                                   [64, 128, 25]             1,250\n",
       "├─Embedding: 1-3                                   [64, 384, 25]             150\n",
       "├─WordEncoder: 1-4                                 [64, 128, 128]            --\n",
       "│    └─FusionLayer: 2-1                            [64, 128, 128]            --\n",
       "│    │    └─Linear: 3-1                            [64, 128, 150]            22,650\n",
       "│    │    └─Sigmoid: 3-2                           [64, 128, 150]            --\n",
       "│    │    └─Linear: 3-3                            [64, 128, 128]            19,328\n",
       "│    └─PositionalEncoding: 2-2                     [64, 128, 128]            --\n",
       "│    │    └─Embedding: 3-4                         [1, 128, 128]             16,384\n",
       "│    └─Dropout: 2-3                                [64, 128, 128]            --\n",
       "│    └─ModuleList: 2-4                             --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-5           [64, 128, 128]            --\n",
       "│    │    │    └─MultiHeadSelfAttention: 4-1       [64, 128, 128]            66,048\n",
       "│    │    │    └─Dropout: 4-2                      [64, 128, 128]            --\n",
       "│    │    │    └─LayerNorm: 4-3                    [64, 128, 128]            256\n",
       "│    │    │    └─PositionwiseFeedForward: 4-4      [64, 128, 128]            131,968\n",
       "│    │    └─TransformerEncoderLayer: 3-6           [64, 128, 128]            --\n",
       "│    │    │    └─MultiHeadSelfAttention: 4-5       [64, 128, 128]            66,048\n",
       "│    │    │    └─Dropout: 4-6                      [64, 128, 128]            --\n",
       "│    │    │    └─LayerNorm: 4-7                    [64, 128, 128]            256\n",
       "│    │    │    └─PositionwiseFeedForward: 4-8      [64, 128, 128]            131,968\n",
       "├─WordAttention: 1-5                               [64, 128]                 --\n",
       "│    └─Linear: 2-5                                 [64, 128, 1]              129\n",
       "├─SentenceEncoder: 1-6                             [2, 32, 128]              --\n",
       "│    └─PositionalEncoding: 2-6                     [2, 32, 128]              --\n",
       "│    │    └─Embedding: 3-7                         [1, 32, 128]              4,096\n",
       "│    └─Dropout: 2-7                                [2, 32, 128]              --\n",
       "│    └─ModuleList: 2-8                             --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-8           [2, 32, 128]              --\n",
       "│    │    │    └─MultiHeadSelfAttention: 4-9       [2, 32, 128]              66,048\n",
       "│    │    │    └─Dropout: 4-10                     [2, 32, 128]              --\n",
       "│    │    │    └─LayerNorm: 4-11                   [2, 32, 128]              256\n",
       "│    │    │    └─PositionwiseFeedForward: 4-12     [2, 32, 128]              131,968\n",
       "│    │    └─TransformerEncoderLayer: 3-9           [2, 32, 128]              --\n",
       "│    │    │    └─MultiHeadSelfAttention: 4-13      [2, 32, 128]              66,048\n",
       "│    │    │    └─Dropout: 4-14                     [2, 32, 128]              --\n",
       "│    │    │    └─LayerNorm: 4-15                   [2, 32, 128]              256\n",
       "│    │    │    └─PositionwiseFeedForward: 4-16     [2, 32, 128]              131,968\n",
       "├─SentenceAttention: 1-7                           [2, 128]                  --\n",
       "│    └─Linear: 2-9                                 [2, 32, 1]                129\n",
       "├─Dropout: 1-8                                     [2, 128]                  --\n",
       "├─Linear: 1-9                                      [2, 20]                   2,580\n",
       "====================================================================================================\n",
       "Total params: 3,911,984\n",
       "Trainable params: 3,911,984\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 224.32\n",
       "====================================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 217.55\n",
       "Params size (MB): 15.65\n",
       "Estimated Total Size (MB): 233.59\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "from model import get_model\n",
    "from torchinfo import summary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = 30522        # Example vocab size (e.g., from BERT tokenizer)\n",
    "pos_vocab_size = 50       # Number of unique POS tags\n",
    "rule_vocab_size = 6       # Number of unique rules (including padding)\n",
    "num_classes = 20          # Number of output classes\n",
    "embed_dim = 100\n",
    "pos_embed_dim = 25\n",
    "rule_embed_dim = 25\n",
    "fusion_dim = 128\n",
    "max_word_len = 128        # Maximum number of tokens per sentence\n",
    "max_sent_len = 32         # Maximum number of sentences per document\n",
    "max_rules_per_word = 3\n",
    "\n",
    "# Instantiate the model\n",
    "model = get_model(\n",
    "    vocab_size=vocab_size,\n",
    "    pos_vocab_size=pos_vocab_size,\n",
    "    rule_vocab_size=rule_vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=embed_dim,\n",
    "    pos_embed_dim=pos_embed_dim,\n",
    "    rule_embed_dim=rule_embed_dim,\n",
    "    fusion_dim=fusion_dim,\n",
    "    max_word_len=max_word_len,\n",
    "    max_sent_len=max_sent_len,\n",
    "    max_rules_per_word=max_rules_per_word\n",
    ")\n",
    "\n",
    "# Create dummy inputs to test the model\n",
    "batch_size = 2\n",
    "num_sentences = max_sent_len\n",
    "seq_length = max_word_len\n",
    "\n",
    "# Dummy input tensors\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, num_sentences, seq_length))\n",
    "attention_mask = torch.ones(batch_size, num_sentences, seq_length, dtype=torch.long)\n",
    "pos_tags = torch.randint(0, pos_vocab_size, (batch_size, num_sentences, seq_length))\n",
    "rules = torch.randint(0, rule_vocab_size, (batch_size, num_sentences, seq_length, max_rules_per_word))\n",
    "sentence_masks = torch.ones(batch_size, num_sentences, dtype=torch.long)\n",
    "# Move model and data to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "pos_tags = pos_tags.to(device)\n",
    "rules = rules.to(device)\n",
    "sentence_masks = sentence_masks.to(device)\n",
    "# Forward pass\n",
    "outputs = model(input_ids, attention_mask, pos_tags, rules, sentence_masks)\n",
    "print(\"Model output shape:\", outputs.shape)\n",
    "# Print model summary\n",
    "summary(model, input_data=(input_ids, attention_mask, pos_tags, rules, sentence_masks), depth=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing model directory for storing checkpoints \n",
    "import shutil \n",
    "shutil.rmtree('/home/ubuntu/Research/Topic_Modelling/SLM2_v2/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Creating datasets and dataloaders...\n",
      "Initializing the model...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:35<00:00, 13.23it/s]\n",
      "Train Loss: 2.7484 | Train Acc: 0.1100\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.44it/s]\n",
      "Test Loss: 2.4701 | Test Acc: 0.1679\n",
      "Best model saved.\n",
      "\n",
      "Epoch 2/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.49it/s]\n",
      "Train Loss: 2.2740 | Train Acc: 0.2228\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.48it/s]\n",
      "Test Loss: 2.1707 | Test Acc: 0.2703\n",
      "Best model saved.\n",
      "\n",
      "Epoch 3/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:35<00:00, 13.48it/s]\n",
      "Train Loss: 1.8951 | Train Acc: 0.3423\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.43it/s]\n",
      "Test Loss: 1.8794 | Test Acc: 0.3515\n",
      "Best model saved.\n",
      "\n",
      "Epoch 4/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.49it/s]\n",
      "Train Loss: 1.5785 | Train Acc: 0.4557\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.40it/s]\n",
      "Test Loss: 1.6533 | Test Acc: 0.4655\n",
      "Best model saved.\n",
      "\n",
      "Epoch 5/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 1.3721 | Train Acc: 0.5304\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.45it/s]\n",
      "Test Loss: 1.5669 | Test Acc: 0.4875\n",
      "Best model saved.\n",
      "\n",
      "Epoch 6/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 1.2175 | Train Acc: 0.5818\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.39it/s]\n",
      "Test Loss: 1.5144 | Test Acc: 0.5310\n",
      "Best model saved.\n",
      "\n",
      "Epoch 7/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 1.0855 | Train Acc: 0.6367\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.45it/s]\n",
      "Test Loss: 1.4635 | Test Acc: 0.5605\n",
      "Best model saved.\n",
      "\n",
      "Epoch 8/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.9732 | Train Acc: 0.6749\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.54it/s]\n",
      "Test Loss: 1.4978 | Test Acc: 0.5650\n",
      "Best model saved.\n",
      "\n",
      "Epoch 9/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 0.8920 | Train Acc: 0.7050\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.55it/s]\n",
      "Test Loss: 1.5753 | Test Acc: 0.5424\n",
      "\n",
      "Epoch 10/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 0.8257 | Train Acc: 0.7269\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.56it/s]\n",
      "Test Loss: 1.4443 | Test Acc: 0.5886\n",
      "Best model saved.\n",
      "\n",
      "Epoch 11/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.7574 | Train Acc: 0.7542\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.57it/s]\n",
      "Test Loss: 1.5908 | Test Acc: 0.5753\n",
      "\n",
      "Epoch 12/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.7121 | Train Acc: 0.7646\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.56it/s]\n",
      "Test Loss: 1.5062 | Test Acc: 0.5958\n",
      "Best model saved.\n",
      "\n",
      "Epoch 13/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.6511 | Train Acc: 0.7873\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.42it/s]\n",
      "Test Loss: 1.6479 | Test Acc: 0.5859\n",
      "\n",
      "Epoch 14/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.6277 | Train Acc: 0.7948\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.43it/s]\n",
      "Test Loss: 1.5192 | Test Acc: 0.5973\n",
      "Best model saved.\n",
      "\n",
      "Epoch 15/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.5802 | Train Acc: 0.8104\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.64it/s]\n",
      "Test Loss: 1.5790 | Test Acc: 0.5968\n",
      "\n",
      "Epoch 16/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.5491 | Train Acc: 0.8225\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.66it/s]\n",
      "Test Loss: 1.6873 | Test Acc: 0.5910\n",
      "\n",
      "Epoch 17/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.5248 | Train Acc: 0.8303\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.66it/s]\n",
      "Test Loss: 1.7128 | Test Acc: 0.5979\n",
      "Best model saved.\n",
      "\n",
      "Epoch 18/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.5047 | Train Acc: 0.8361\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.64it/s]\n",
      "Test Loss: 1.6908 | Test Acc: 0.5926\n",
      "\n",
      "Epoch 19/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.4727 | Train Acc: 0.8470\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.58it/s]\n",
      "Test Loss: 1.7763 | Test Acc: 0.5859\n",
      "\n",
      "Epoch 20/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.4601 | Train Acc: 0.8529\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.61it/s]\n",
      "Test Loss: 1.7325 | Test Acc: 0.6058\n",
      "Best model saved.\n",
      "\n",
      "Epoch 21/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.4475 | Train Acc: 0.8583\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.49it/s]\n",
      "Test Loss: 1.6689 | Test Acc: 0.6037\n",
      "\n",
      "Epoch 22/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.4180 | Train Acc: 0.8670\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.65it/s]\n",
      "Test Loss: 1.8092 | Test Acc: 0.5979\n",
      "\n",
      "Epoch 23/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.4029 | Train Acc: 0.8734\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.60it/s]\n",
      "Test Loss: 1.8455 | Test Acc: 0.5958\n",
      "\n",
      "Epoch 24/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.3872 | Train Acc: 0.8787\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.66it/s]\n",
      "Test Loss: 1.8812 | Test Acc: 0.5958\n",
      "\n",
      "Epoch 25/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.3745 | Train Acc: 0.8821\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.59it/s]\n",
      "Test Loss: 1.8329 | Test Acc: 0.6037\n",
      "\n",
      "Epoch 26/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.3844 | Train Acc: 0.8770\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.58it/s]\n",
      "Test Loss: 1.7916 | Test Acc: 0.5997\n",
      "\n",
      "Epoch 27/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.3689 | Train Acc: 0.8835\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.64it/s]\n",
      "Test Loss: 1.8857 | Test Acc: 0.6027\n",
      "\n",
      "Epoch 28/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.3545 | Train Acc: 0.8891\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.62it/s]\n",
      "Test Loss: 1.8641 | Test Acc: 0.5997\n",
      "\n",
      "Epoch 29/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.3436 | Train Acc: 0.8925\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.64it/s]\n",
      "Test Loss: 1.9501 | Test Acc: 0.6085\n",
      "Best model saved.\n",
      "\n",
      "Epoch 30/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.50it/s]\n",
      "Train Loss: 0.3369 | Train Acc: 0.8933\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.71it/s]\n",
      "Test Loss: 1.9865 | Test Acc: 0.6058\n",
      "\n",
      "Epoch 31/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.3418 | Train Acc: 0.8926\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.58it/s]\n",
      "Test Loss: 1.8432 | Test Acc: 0.6053\n",
      "\n",
      "Epoch 32/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.3439 | Train Acc: 0.8912\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.62it/s]\n",
      "Test Loss: 1.9295 | Test Acc: 0.6183\n",
      "Best model saved.\n",
      "\n",
      "Epoch 33/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.3162 | Train Acc: 0.9019\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.62it/s]\n",
      "Test Loss: 1.9507 | Test Acc: 0.6085\n",
      "\n",
      "Epoch 34/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2978 | Train Acc: 0.9087\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.68it/s]\n",
      "Test Loss: 2.0034 | Test Acc: 0.6162\n",
      "\n",
      "Epoch 35/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2940 | Train Acc: 0.9087\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.65it/s]\n",
      "Test Loss: 1.9833 | Test Acc: 0.6032\n",
      "\n",
      "Epoch 36/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2970 | Train Acc: 0.9064\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.60it/s]\n",
      "Test Loss: 1.8977 | Test Acc: 0.6210\n",
      "Best model saved.\n",
      "\n",
      "Epoch 37/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2827 | Train Acc: 0.9154\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.69it/s]\n",
      "Test Loss: 2.0308 | Test Acc: 0.6114\n",
      "\n",
      "Epoch 38/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2823 | Train Acc: 0.9116\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.63it/s]\n",
      "Test Loss: 1.9442 | Test Acc: 0.6188\n",
      "\n",
      "Epoch 39/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2846 | Train Acc: 0.9130\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.70it/s]\n",
      "Test Loss: 2.0255 | Test Acc: 0.6048\n",
      "\n",
      "Epoch 40/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2780 | Train Acc: 0.9124\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.62it/s]\n",
      "Test Loss: 2.0437 | Test Acc: 0.6114\n",
      "\n",
      "Epoch 41/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2662 | Train Acc: 0.9170\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.61it/s]\n",
      "Test Loss: 2.1509 | Test Acc: 0.6000\n",
      "\n",
      "Epoch 42/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2598 | Train Acc: 0.9176\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.61it/s]\n",
      "Test Loss: 2.0168 | Test Acc: 0.6122\n",
      "\n",
      "Epoch 43/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2526 | Train Acc: 0.9199\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.68it/s]\n",
      "Test Loss: 1.9979 | Test Acc: 0.6225\n",
      "Best model saved.\n",
      "\n",
      "Epoch 44/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2586 | Train Acc: 0.9201\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.62it/s]\n",
      "Test Loss: 2.0716 | Test Acc: 0.6045\n",
      "\n",
      "Epoch 45/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.52it/s]\n",
      "Train Loss: 0.2389 | Train Acc: 0.9259\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.65it/s]\n",
      "Test Loss: 2.1307 | Test Acc: 0.6204\n",
      "\n",
      "Epoch 46/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2451 | Train Acc: 0.9248\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.53it/s]\n",
      "Test Loss: 2.1892 | Test Acc: 0.6117\n",
      "\n",
      "Epoch 47/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2338 | Train Acc: 0.9259\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.55it/s]\n",
      "Test Loss: 2.2027 | Test Acc: 0.6058\n",
      "\n",
      "Epoch 48/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2387 | Train Acc: 0.9247\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.61it/s]\n",
      "Test Loss: 2.1011 | Test Acc: 0.6119\n",
      "\n",
      "Epoch 49/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2511 | Train Acc: 0.9244\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.60it/s]\n",
      "Test Loss: 1.9425 | Test Acc: 0.6159\n",
      "\n",
      "Epoch 50/50\n",
      "Training: 100%|███████████████████████████████| 472/472 [00:34<00:00, 13.51it/s]\n",
      "Train Loss: 0.2732 | Train Acc: 0.9171\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 38.67it/s]\n",
      "Test Loss: 1.9363 | Test Acc: 0.6228\n",
      "Best model saved.\n",
      "\n",
      "Training completed.\n",
      "Best Test Accuracy: 0.6228\n"
     ]
    }
   ],
   "source": [
    "!python train.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Creating dataset and dataloader...\n",
      "Initializing the model...\n",
      "/home/ubuntu/Research/Topic_Modelling/SLM2_v2/evaluate.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "Model loaded successfully.\n",
      "Evaluating the model on the test set...\n",
      "Evaluating: 100%|█████████████████████████████| 118/118 [00:03<00:00, 32.28it/s]\n",
      "\n",
      "Test Loss: 1.9363 | Test Accuracy: 0.6228\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.49      0.51      0.50       151\n",
      "           comp.graphics       0.54      0.54      0.54       202\n",
      " comp.os.ms-windows.misc       0.58      0.59      0.59       195\n",
      "comp.sys.ibm.pc.hardware       0.58      0.54      0.56       183\n",
      "   comp.sys.mac.hardware       0.71      0.63      0.67       205\n",
      "          comp.windows.x       0.73      0.77      0.75       215\n",
      "            misc.forsale       0.65      0.65      0.65       193\n",
      "               rec.autos       0.42      0.68      0.52       196\n",
      "         rec.motorcycles       0.73      0.55      0.63       168\n",
      "      rec.sport.baseball       0.78      0.71      0.74       211\n",
      "        rec.sport.hockey       0.81      0.84      0.82       198\n",
      "               sci.crypt       0.72      0.72      0.72       201\n",
      "         sci.electronics       0.47      0.52      0.49       202\n",
      "                 sci.med       0.76      0.66      0.71       194\n",
      "               sci.space       0.68      0.72      0.70       189\n",
      "  soc.religion.christian       0.70      0.51      0.59       202\n",
      "      talk.politics.guns       0.63      0.62      0.62       188\n",
      "   talk.politics.mideast       0.79      0.75      0.77       182\n",
      "      talk.politics.misc       0.47      0.41      0.44       159\n",
      "      talk.religion.misc       0.32      0.37      0.34       136\n",
      "\n",
      "                accuracy                           0.62      3770\n",
      "               macro avg       0.63      0.61      0.62      3770\n",
      "            weighted avg       0.64      0.62      0.63      3770\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 77   0   3   0   0   3   0   5   2   3   0   4   4   2   3  13   1   7\n",
      "    3  21]\n",
      " [  4 110  21   8   1  20   3   6   1   1   0   4  10   2   6   2   0   0\n",
      "    2   1]\n",
      " [  1  18 116  18   0   8   9   8   0   1   2   0   7   1   2   1   0   1\n",
      "    1   1]\n",
      " [  0  12  27  98  14   5  12   2   1   2   1   3   4   1   0   1   0   0\n",
      "    0   0]\n",
      " [  0  15   4  12 129   0   7  20   0   0   0   3  11   0   1   0   0   1\n",
      "    1   1]\n",
      " [  0  18   9   0   0 165   1   5   0   0   0   3   8   2   3   1   0   0\n",
      "    0   0]\n",
      " [  0   5   5  12   9   0 126  11   1   2   2   5   7   0   1   3   2   1\n",
      "    1   0]\n",
      " [  0   1   1   2   6   0   6 134   9   6   0   0  11   4   6   0   1   3\n",
      "    4   2]\n",
      " [  4   1   0   1   1   2   4  25  93   2   2   0  10   2   1   1   4   3\n",
      "    9   3]\n",
      " [  2   1   0   6   0   1   3  19   3 150  11   1   0   0   3   0   2   1\n",
      "    5   3]\n",
      " [  1   0   0   0   1   2   2   7   2   7 167   0   0   0   4   0   3   0\n",
      "    1   1]\n",
      " [  1   3   3   1   0   1   3   5   1   2   1 144   7   1  10   0  11   2\n",
      "    3   2]\n",
      " [  3  10   3   7  14   6  11  21   0   3   0   7 105   6   2   0   0   2\n",
      "    2   0]\n",
      " [  3   1   0   1   2   7   0   5   1   1   1   2  20 128   6   0   4   0\n",
      "    6   6]\n",
      " [  1   4   2   1   1   1   1  11   2   0   4   5   9   4 136   1   1   0\n",
      "    5   0]\n",
      " [ 22   3   2   0   1   2   0   4   1   1   1   4   4   4   0 103   2   6\n",
      "    3  39]\n",
      " [  2   0   1   0   0   1   0   9   3   1   4   8   1   1   4   2 116   3\n",
      "   18  14]\n",
      " [  8   0   1   1   1   0   1   6   3   2   2   2   1   1   0   3   6 136\n",
      "    5   3]\n",
      " [  3   1   0   1   1   0   3  13   1   6   8   2   2  10  10   1  19   6\n",
      "   65   7]\n",
      " [ 25   1   1   1   1   2   1   5   3   3   1   3   3   0   3  15  12   1\n",
      "    5  50]]\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions \n",
    "##### Run in Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Research/Topic_Modelling/SLM2_v2/predict.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
      "Model loaded successfully.\n",
      "\n",
      "Enter text to classify (type 'exit' to quit):\n",
      "\n",
      ">> ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/Research/Topic_Modelling/SLM2_v2/predict.py\", line 254, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/Research/Topic_Modelling/SLM2_v2/predict.py\", line 223, in main\n",
      "    user_input = input(\">> \")\n",
      "                 ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
